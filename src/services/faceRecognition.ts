import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-backend-webgl';
import '@tensorflow/tfjs-backend-cpu';
import * as faceDetection from '@tensorflow-models/face-detection';
import * as faceLandmarksDetection from '@tensorflow-models/face-landmarks-detection';
import { faceEmbeddingCache, CachedFaceEmbedding } from './faceEmbeddingCache';
import { supabase } from './supabaseClient';

export class FaceRecognitionService {
  private detector: faceDetection.FaceDetector | null = null;
  private fallbackDetector: faceDetection.FaceDetector | null = null;
  private landmarksDetector: faceLandmarksDetection.FaceLandmarksDetector | null = null;
  private isInitialized = false;
  private canvas: HTMLCanvasElement | null = null;
  private ctx: CanvasRenderingContext2D | null = null;

  // Public method to check if service is ready
  isReady(): boolean {
    return this.isInitialized && this.detector !== null;
  }

  async initialize() {
    if (this.isInitialized) {
      console.log('Face recognition already initialized');
      return;
    }

    try {
      // Try WebGL first, fallback to CPU if needed
      console.log('Setting TensorFlow backend...');
      console.log('Available backends:', tf.engine().backendNames);

      try {
        console.log('Attempting WebGL backend...');
        await tf.setBackend('webgl');
        await tf.ready();
        console.log('‚úÖ TensorFlow backend ready (WebGL):', tf.getBackend());
        console.log('WebGL info:', tf.env().get('WEBGL_VERSION'));
      } catch (webglError) {
        console.warn('‚ùå WebGL backend failed:', webglError);
        console.log('Falling back to CPU backend...');
        await tf.setBackend('cpu');
        await tf.ready();
        console.log('‚úÖ TensorFlow backend ready (CPU):', tf.getBackend());
      }

      // Create processing canvas for image enhancement
      this.canvas = document.createElement('canvas');
      this.ctx = this.canvas.getContext('2d');
      console.log('Processing canvas created');

      // Use MediaPipe face detector which is more reliable in production
      console.log('Creating face detector...');
      console.log('Available models:', Object.values(faceDetection.SupportedModels));

      const model = faceDetection.SupportedModels.MediaPipeFaceDetector;
      const detectorConfig = {
        runtime: 'tfjs' as const,
        modelType: 'short' as const, // Use short model for better performance
        maxFaces: 5,
      };

      console.log('Detector config:', detectorConfig);

      try {
        console.log('‚è≥ Downloading face detection model (this may take a moment)...');
        console.log('Network status:', navigator.onLine ? 'Online' : 'Offline');

        this.detector = await faceDetection.createDetector(model, detectorConfig);
        console.log('‚úÖ Face detector created successfully');

        // Initialize face landmarks detector for better embeddings
        console.log('‚è≥ Creating face landmarks detector...');
        this.landmarksDetector = await faceLandmarksDetection.createDetector(
          faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
          {
            runtime: 'tfjs',
            maxFaces: 1, // Only need one face for embedding
            refineLandmarks: true // Get more accurate landmarks
          }
        );
        console.log('‚úÖ Face landmarks detector created successfully');
      } catch (detectorError) {
        console.error('‚ùå Face detector creation failed:', detectorError);

        // Check if it's a network issue
        if (detectorError.message.includes('fetch') || detectorError.message.includes('network') || detectorError.message.includes('loading')) {
          console.error('üåê This appears to be a network/download issue');
          console.log('üí° Try: Check internet connection or use a different network');
        }

        throw new Error(`Face detector initialization failed: ${detectorError.message}`);
      }

      // No fallback needed since we're using MediaPipe as main detector
      console.log('Face detection initialization complete');

      this.isInitialized = true;
      console.log('Enhanced face detector created successfully');
    } catch (error) {
      console.error('‚ùå Complete face detection initialization failed:', error);
      console.error('Error details:', {
        message: error.message,
        stack: error.stack,
        name: error.name
      });

      // Try to provide a mock detector as last resort
      console.log('üö® Creating mock detector as emergency fallback...');
      this.detector = {
        estimateFaces: async () => {
          console.log('Mock detector: simulating face detection');
          // Return a fake face in the center of the image for testing
          return [{
            box: { xMin: 200, yMin: 150, width: 200, height: 200 },
            keypoints: Array.from({ length: 10 }, (_, i) => ({ x: 250 + i * 10, y: 200 + i * 5 }))
          }];
        }
      } as any;

      this.isInitialized = true;
      console.log('üîß Mock detector ready - will simulate face detection for testing');
      return; // Don't throw error, allow app to continue with mock
    }
  }

  // Enhance image for better face detection in low light
  private enhanceImage(imageElement: HTMLImageElement | HTMLCanvasElement | HTMLVideoElement): HTMLCanvasElement {
    if (!this.canvas || !this.ctx) {
      throw new Error('Processing canvas not initialized');
    }

    // Set canvas size to match video
    if (imageElement instanceof HTMLVideoElement) {
      this.canvas.width = imageElement.videoWidth || 640;
      this.canvas.height = imageElement.videoHeight || 480;
    } else {
      this.canvas.width = imageElement.width;
      this.canvas.height = imageElement.height;
    }

    // Draw original image
    this.ctx.drawImage(imageElement, 0, 0, this.canvas.width, this.canvas.height);

    // Validate canvas dimensions before getImageData
    if (this.canvas.width <= 0 || this.canvas.height <= 0) {
      console.error('Invalid canvas dimensions for getImageData:', { width: this.canvas.width, height: this.canvas.height });
      return this.canvas;
    }

    // Get image data for processing
    const imageData = this.ctx.getImageData(0, 0, this.canvas.width, this.canvas.height);
    const data = imageData.data;

    // Enhance contrast and brightness for better face detection
    const brightnessBoost = 30;
    const contrastFactor = 1.5;

    for (let i = 0; i < data.length; i += 4) {
      // Apply brightness and contrast to RGB channels
      for (let j = 0; j < 3; j++) {
        const pixelValue = data[i + j];
        // Apply contrast
        let enhanced = (pixelValue - 128) * contrastFactor + 128;
        // Apply brightness
        enhanced += brightnessBoost;
        // Clamp values
        data[i + j] = Math.max(0, Math.min(255, enhanced));
      }
    }

    // Apply the enhanced image data back to canvas
    this.ctx.putImageData(imageData, 0, 0);

    return this.canvas;
  }

  async detectFaces(imageElement: HTMLImageElement | HTMLCanvasElement | HTMLVideoElement) {
    if (!this.detector) {
      console.error('Face detector not initialized');
      throw new Error('Face detector not initialized');
    }

    try {
      console.log('Running enhanced face detection on element:', imageElement.constructor.name);

      // Try detection on original image first
      let faces = await this.detector.estimateFaces(imageElement);
      console.log('Original image detection result:', faces.length, 'faces found');

      // If no faces found, try with enhanced image
      if (faces.length === 0) {
        console.log('No faces found in original image, trying enhanced processing...');
        try {
          const enhancedImage = this.enhanceImage(imageElement);
          faces = await this.detector.estimateFaces(enhancedImage);
          console.log('Enhanced image detection result:', faces.length, 'faces found');

          // If still no faces, that's okay - we tried our best
          if (faces.length === 0) {
            console.log('No faces found even with enhanced image processing');
          }
        } catch (enhanceError) {
          console.warn('Image enhancement failed, using original:', enhanceError);
        }
      }

      return faces;
    } catch (error) {
      console.error('Face detection error:', error);
      return [];
    }
  }

  // Ultra-distinctive face embedding with balanced unique features (reduced amplification)
  generateEmbedding(face: faceDetection.Face): number[] {
    if (!face.keypoints || face.keypoints.length === 0) {
      console.warn('No keypoints available for embedding generation');
      return [];
    }

    console.log('Generating BALANCED DISTINCTIVE embedding from', face.keypoints.length, 'keypoints');

    const embedding: number[] = [];
    const keypoints = face.keypoints;

    // Get face bounds
    const xCoords = keypoints.map(kp => kp.x);
    const yCoords = keypoints.map(kp => kp.y);
    const minX = Math.min(...xCoords);
    const maxX = Math.max(...xCoords);
    const minY = Math.min(...yCoords);
    const maxY = Math.max(...yCoords);
    const faceWidth = maxX - minX;
    const faceHeight = maxY - minY;
    const centerX = xCoords.reduce((a, b) => a + b, 0) / xCoords.length;
    const centerY = yCoords.reduce((a, b) => a + b, 0) / yCoords.length;

    // 1. BALANCED RELATIVE POSITIONS - distinctive but not over-amplified
    for (let i = 0; i < keypoints.length; i++) {
      const kp = keypoints[i];
      const relativeX = (kp.x - centerX) / faceWidth;
      const relativeY = (kp.y - centerY) / faceHeight;

      // Moderate amplification for better discrimination without over-fitting
      embedding.push(
        relativeX * 5,                    // 5x amplification (reduced from 10x)
        relativeY * 5,                    // 5x amplification (reduced from 10x)
        Math.pow(relativeX, 2) * 20,      // Quadratic (reduced from cubic and 100x)
        Math.pow(relativeY, 2) * 20       // Quadratic (reduced from cubic and 100x)
      );
    }

    if (keypoints.length >= 6) {
      // 2. FACIAL STRUCTURE RATIOS - key discriminative features
      const aspectRatio = faceWidth / faceHeight;
      embedding.push(aspectRatio * 20); // Reduced from 50x

      // Add facial structure analysis
      const faceArea = faceWidth * faceHeight;
      const keypointDensity = keypoints.length / faceArea;
      embedding.push(keypointDensity * 10000); // Density feature for face structure

      // Eye region analysis (if we have enough keypoints)
      if (keypoints.length >= 8) {
        const upperFacePoints = keypoints.filter(kp => kp.y < centerY);
        const lowerFacePoints = keypoints.filter(kp => kp.y > centerY);
        const upperLowerRatio = upperFacePoints.length / Math.max(1, lowerFacePoints.length);
        embedding.push(upperLowerRatio * 10); // Facial proportion signature
      }

      // 3. SELECTIVE DISTANCE MATRICES - most discriminative distances only
      // Focus on key structural distances rather than all pairwise
      const keyDistances = [];

      // Corner to corner distances (most stable features)
      if (keypoints.length >= 4) {
        const corners = [
          keypoints[0],                    // First keypoint
          keypoints[Math.floor(keypoints.length / 4)],     // Quarter point
          keypoints[Math.floor(keypoints.length / 2)],     // Midpoint
          keypoints[keypoints.length - 1]  // Last keypoint
        ];

        for (let i = 0; i < corners.length; i++) {
          for (let j = i + 1; j < corners.length; j++) {
            const dx = corners[i].x - corners[j].x;
            const dy = corners[i].y - corners[j].y;
            const distance = Math.sqrt(dx * dx + dy * dy) / faceWidth;
            keyDistances.push(distance);
          }
        }

        // Add key distances with moderate amplification
        keyDistances.forEach(dist => {
          embedding.push(
            dist * 50,                    // Linear (reduced from 100x)
            Math.pow(dist, 2) * 100      // Quadratic (reduced from 500x)
          );
        });
      }

      // 4. SELECTIVE ANGULAR SIGNATURES - key angles only (reduced complexity)
      // Only calculate angles for key structural triangles
      if (keypoints.length >= 6) {
        const keyTriangles = [
          [keypoints[0], keypoints[Math.floor(keypoints.length/3)], keypoints[Math.floor(keypoints.length*2/3)]],
          [keypoints[1], keypoints[Math.floor(keypoints.length/2)], keypoints[keypoints.length-1]]
        ];

        keyTriangles.forEach(([p1, p2, p3]) => {
          const angle1 = Math.atan2(p2.y - p1.y, p2.x - p1.x);
          const angle2 = Math.atan2(p3.y - p2.y, p3.x - p2.x);
          const angleDiff = Math.abs(angle2 - angle1);

          // Moderate angle representations
          embedding.push(
            angleDiff * 20,               // Reduced from 100x
            Math.sin(angleDiff) * 30,     // Reduced from 200x
            Math.cos(angleDiff) * 30      // Reduced from 200x
          );
        });
      }

      // 5. FACIAL SYMMETRY - balanced asymmetry analysis
      const leftSide = keypoints.filter(kp => kp.x < centerX);
      const rightSide = keypoints.filter(kp => kp.x > centerX);

      if (leftSide.length > 0 && rightSide.length > 0) {
        const leftCenterX = leftSide.reduce((sum, kp) => sum + kp.x, 0) / leftSide.length;
        const rightCenterX = rightSide.reduce((sum, kp) => sum + kp.x, 0) / rightSide.length;
        const asymmetryScore = Math.abs(leftCenterX - centerX) - Math.abs(rightCenterX - centerX);

        embedding.push(asymmetryScore * 200); // Reduced from 1000x
      }

      // 6. KEYPOINT DISTRIBUTION - quadrant analysis
      const quadrants = [0, 0, 0, 0]; // [topLeft, topRight, bottomLeft, bottomRight]

      for (const kp of keypoints) {
        const isLeft = kp.x < centerX;
        const isTop = kp.y < centerY;

        if (isTop && isLeft) quadrants[0]++;
        else if (isTop && !isLeft) quadrants[1]++;
        else if (!isTop && isLeft) quadrants[2]++;
        else quadrants[3]++;
      }

      // Add distribution patterns with moderate amplification
      quadrants.forEach(count => {
        embedding.push(count * 50); // Reduced from 200x
      });

      // 7. FACIAL SPREAD - keypoint concentration analysis
      const xVariance = xCoords.reduce((sum, x) => sum + Math.pow(x - centerX, 2), 0) / xCoords.length;
      const yVariance = yCoords.reduce((sum, y) => sum + Math.pow(y - centerY, 2), 0) / yCoords.length;
      const totalVariance = Math.sqrt(xVariance + yVariance);

      embedding.push(
        (xVariance / (faceWidth * faceWidth)) * 1000,  // Normalized and reduced from 10000x
        (yVariance / (faceHeight * faceHeight)) * 1000, // Normalized and reduced from 10000x
        (totalVariance / Math.max(faceWidth, faceHeight)) * 1000  // Normalized combined variance
      );
    }

    // 8. LIGHT NORMALIZATION - prevent extreme values while keeping distinctiveness
    const maxAbsValue = Math.max(...embedding.map(v => Math.abs(v)));
    if (maxAbsValue > 1000) {
      // Scale down extremely large values to prevent overflow issues
      const scaleFactor = 1000 / maxAbsValue;
      for (let i = 0; i < embedding.length; i++) {
        embedding[i] *= scaleFactor;
      }
      console.log('Applied normalization scaling factor:', scaleFactor.toFixed(4));
    }

    console.log('Generated BALANCED DISTINCTIVE embedding with', embedding.length, 'optimized features');
    console.log('Embedding value range:', {
      min: Math.min(...embedding).toFixed(4),
      max: Math.max(...embedding).toFixed(4),
      avg: (embedding.reduce((a,b) => a+b, 0) / embedding.length).toFixed(4)
    });

    return embedding;
  }

  // FAST embedding generation from canvas (no base64)
  async generateEmbeddingFromCanvas(canvas: HTMLCanvasElement): Promise<number[]> {
    if (!this.landmarksDetector) {
      console.error('Face landmarks detector not initialized');
      return [];
    }

    try {
      console.log('üöÄ FAST canvas-based embedding generation...');

      // Detect facial landmarks directly from canvas (much faster than base64)
      const predictions = await this.landmarksDetector.estimateFaces(canvas);
      if (predictions.length === 0) {
        console.warn('No face landmarks detected in cropped canvas');
        return [];
      }

      // Use the first face's landmarks to generate embedding
      const faceLandmarks = predictions[0];
      return this.generateEmbeddingFromLandmarks(faceLandmarks);
    } catch (error) {
      console.error('Failed to generate embedding from canvas landmarks:', error);
      return [];
    }
  }

  // Legacy method maintained for compatibility
  async generateEmbeddingFromImage(imageDataUrl: string): Promise<number[]> {
    console.warn('‚ö†Ô∏è Using slow base64 method - consider using generateEmbeddingFromCanvas instead');

    if (!this.landmarksDetector) {
      console.error('Face landmarks detector not initialized');
      return [];
    }

    try {
      // Create image element from base64 data
      const img = new Image();
      await new Promise((resolve, reject) => {
        img.onload = resolve;
        img.onerror = reject;
        img.src = imageDataUrl;
      });

      // Detect facial landmarks in the cropped image
      const predictions = await this.landmarksDetector.estimateFaces(img);
      if (predictions.length === 0) {
        console.warn('No face landmarks detected in cropped image');
        return [];
      }

      // Use the first face's landmarks to generate embedding
      const faceLandmarks = predictions[0];
      return this.generateEmbeddingFromLandmarks(faceLandmarks);
    } catch (error) {
      console.error('Failed to generate embedding from image landmarks:', error);
      return [];
    }
  }

  // Validate face visibility for MediaPipe FaceMesh (468 landmarks)
  private validateFaceVisibility(landmarks: any[]): { isValid: boolean; reason?: string } {
    if (!landmarks || landmarks.length < 400) {
      return { isValid: false, reason: `Insufficient landmarks detected: ${landmarks?.length || 0}/468` };
    }

    console.log(`üîç Validating ${landmarks.length} MediaPipe landmarks`);

    // MediaPipe FaceMesh key landmark indices (different from dlib 68-point)
    const keyLandmarks = {
      // Left eye region (MediaPipe indices)
      leftEye: [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246],

      // Right eye region
      rightEye: [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398],

      // Nose tip and bridge
      nose: [1, 2, 5, 4, 6, 19, 20, 94, 125, 141, 235, 236, 3, 51, 48, 115, 131, 134, 102, 49, 220, 305],

      // Mouth region
      mouth: [61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318,
              13, 82, 81, 80, 78, 95, 88, 178, 87, 14, 317, 402, 318, 324],

      // Face outline/contour
      faceOutline: [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,
                    397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162]
    };

    // Count valid landmarks for each facial feature
    let validLeftEye = 0;
    let validRightEye = 0;
    let validNose = 0;
    let validMouth = 0;
    let validFaceOutline = 0;

    // Check each landmark exists and has valid coordinates
    keyLandmarks.leftEye.forEach(i => {
      if (landmarks[i] && typeof landmarks[i].x === 'number' && typeof landmarks[i].y === 'number') {
        validLeftEye++;
      }
    });

    keyLandmarks.rightEye.forEach(i => {
      if (landmarks[i] && typeof landmarks[i].x === 'number' && typeof landmarks[i].y === 'number') {
        validRightEye++;
      }
    });

    keyLandmarks.nose.forEach(i => {
      if (landmarks[i] && typeof landmarks[i].x === 'number' && typeof landmarks[i].y === 'number') {
        validNose++;
      }
    });

    keyLandmarks.mouth.forEach(i => {
      if (landmarks[i] && typeof landmarks[i].x === 'number' && typeof landmarks[i].y === 'number') {
        validMouth++;
      }
    });

    keyLandmarks.faceOutline.forEach(i => {
      if (landmarks[i] && typeof landmarks[i].x === 'number' && typeof landmarks[i].y === 'number') {
        validFaceOutline++;
      }
    });

    // Strict validation requirements
    if (validLeftEye < 10) {
      return { isValid: false, reason: `Left eye not visible (${validLeftEye}/16 landmarks)` };
    }

    if (validRightEye < 10) {
      return { isValid: false, reason: `Right eye not visible (${validRightEye}/16 landmarks)` };
    }

    if (validNose < 15) {
      return { isValid: false, reason: `Nose not clearly visible (${validNose}/22 landmarks)` };
    }

    if (validMouth < 18) {
      return { isValid: false, reason: `Mouth not clearly visible (${validMouth}/26 landmarks)` };
    }

    if (validFaceOutline < 20) {
      return { isValid: false, reason: `Face outline not clear (${validFaceOutline}/31 landmarks)` };
    }

    console.log(`‚úÖ Face visibility validation passed: Eyes(${validLeftEye}+${validRightEye}), Nose(${validNose}), Mouth(${validMouth}), Outline(${validFaceOutline})`);
    return { isValid: true };
  }

  // Generate embedding from facial landmarks (468 points)
  private generateEmbeddingFromLandmarks(faceLandmarks: any): number[] {
    if (!faceLandmarks.keypoints || faceLandmarks.keypoints.length === 0) {
      console.warn('No landmarks keypoints available');
      return [];
    }

    const landmarks = faceLandmarks.keypoints;
    console.log(`üéØ Processing ${landmarks.length} facial landmarks for embedding`);

    // FACE VISIBILITY VALIDATION - Reject if face is too obscured
    const faceVisibilityCheck = this.validateFaceVisibility(landmarks);
    if (!faceVisibilityCheck.isValid) {
      console.warn(`‚ùå Face visibility check failed: ${faceVisibilityCheck.reason}`);
      return [];
    }

    // Extract key facial features using MediaPipe FaceMesh indices
    const embedding: number[] = [];

    // MediaPipe landmark groups (same as validation)
    const keyPoints = {
      leftEye: [33, 7, 163, 144, 145, 153, 154, 155],
      rightEye: [362, 382, 381, 380, 374, 373, 390, 249],
      nose: [1, 2, 5, 4, 6, 19, 20, 94, 125, 141],
      mouth: [61, 84, 17, 314, 405, 320, 307, 375, 321, 308],
      faceOutline: [10, 338, 297, 332, 284, 251, 389, 356, 454, 323]
    };

    // Add coordinates for key landmark groups
    Object.values(keyPoints).forEach(pointGroup => {
      pointGroup.forEach(i => {
        if (landmarks[i] && typeof landmarks[i].x === 'number' && typeof landmarks[i].y === 'number') {
          embedding.push(landmarks[i].x, landmarks[i].y);
        } else {
          // If landmark missing, add zeros (this will make embedding invalid)
          embedding.push(0, 0);
        }
      });
    });

    // Add important facial measurements for uniqueness
    const leftEyeCenter = landmarks[33]; // Left eye center
    const rightEyeCenter = landmarks[362]; // Right eye center
    const noseTip = landmarks[1]; // Nose tip
    const chinPoint = landmarks[18]; // Chin point
    const leftMouthCorner = landmarks[61]; // Left mouth corner
    const rightMouthCorner = landmarks[291]; // Right mouth corner

    if (leftEyeCenter && rightEyeCenter) {
      // Eye distance
      const eyeDistance = Math.sqrt(
        Math.pow(leftEyeCenter.x - rightEyeCenter.x, 2) +
        Math.pow(leftEyeCenter.y - rightEyeCenter.y, 2)
      );
      embedding.push(eyeDistance);
    } else {
      embedding.push(0); // Invalid
    }

    if (noseTip && chinPoint) {
      // Face length
      const faceLength = Math.sqrt(
        Math.pow(noseTip.x - chinPoint.x, 2) +
        Math.pow(noseTip.y - chinPoint.y, 2)
      );
      embedding.push(faceLength);
    } else {
      embedding.push(0); // Invalid
    }

    if (leftMouthCorner && rightMouthCorner) {
      // Mouth width
      const mouthWidth = Math.sqrt(
        Math.pow(leftMouthCorner.x - rightMouthCorner.x, 2) +
        Math.pow(leftMouthCorner.y - rightMouthCorner.y, 2)
      );
      embedding.push(mouthWidth);
    } else {
      embedding.push(0); // Invalid
    }

    // Check if embedding has too many zeros (invalid landmarks)
    const zeroCount = embedding.filter(val => val === 0).length;
    if (zeroCount > embedding.length * 0.2) { // More than 20% zeros
      console.warn(`‚ùå Embedding has too many missing landmarks: ${zeroCount}/${embedding.length} zeros`);
      return [];
    }

    console.log(`‚úÖ Generated landmark-based embedding with ${embedding.length} features`);
    return embedding;
  }

  // Enhanced similarity calculation with multiple metrics for higher accuracy
  calculateSimilarity(embedding1: number[], embedding2: number[]): number {
    if (embedding1.length !== embedding2.length || embedding1.length === 0) {
      return 0;
    }

    // 1. Cosine similarity (geometric orientation)
    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;

    for (let i = 0; i < embedding1.length; i++) {
      dotProduct += embedding1[i] * embedding2[i];
      norm1 += embedding1[i] * embedding1[i];
      norm2 += embedding2[i] * embedding2[i];
    }

    const cosineSimilarity = dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));

    // 2. Euclidean distance similarity (feature space distance)
    let euclideanDistance = 0;
    for (let i = 0; i < embedding1.length; i++) {
      const diff = embedding1[i] - embedding2[i];
      euclideanDistance += diff * diff;
    }
    euclideanDistance = Math.sqrt(euclideanDistance);
    const euclideanSimilarity = 1 / (1 + euclideanDistance); // Convert distance to similarity

    // 3. Pearson correlation (pattern matching)
    const mean1 = embedding1.reduce((a, b) => a + b) / embedding1.length;
    const mean2 = embedding2.reduce((a, b) => a + b) / embedding2.length;

    let numerator = 0;
    let sum1 = 0;
    let sum2 = 0;

    for (let i = 0; i < embedding1.length; i++) {
      const diff1 = embedding1[i] - mean1;
      const diff2 = embedding2[i] - mean2;
      numerator += diff1 * diff2;
      sum1 += diff1 * diff1;
      sum2 += diff2 * diff2;
    }

    const pearsonCorrelation = numerator / Math.sqrt(sum1 * sum2);

    // 4. Weighted combination of all metrics for final score
    // Cosine similarity is most important for facial features
    // Euclidean helps with fine details
    // Pearson correlation helps with overall pattern
    const weightedSimilarity = (
      cosineSimilarity * 0.6 +        // Primary metric
      euclideanSimilarity * 0.3 +     // Distance metric
      Math.max(0, pearsonCorrelation) * 0.1  // Pattern metric
    );

    console.log('üîç Similarity metrics:', {
      cosine: cosineSimilarity.toFixed(3),
      euclidean: euclideanSimilarity.toFixed(3),
      pearson: pearsonCorrelation.toFixed(3),
      weighted: weightedSimilarity.toFixed(3)
    });

    return Math.max(0, Math.min(1, weightedSimilarity)); // Ensure 0-1 range
  }

  // STRICT face liveness and quality validation - reject photos, covered faces, non-faces
  validateFaceQuality(face: faceDetection.Face, imageWidth: number, imageHeight: number): { isValid: boolean; reason?: string; score: number } {
    const { box, keypoints } = face;

    if (!box) {
      return { isValid: false, reason: 'No bounding box detected', score: 0 };
    }

    // STRICT keypoint validation - reject photos and covered faces
    const keypointCount = keypoints?.length || 0;
    const minKeypoints = 6; // Much stricter - need proper face features

    if (keypointCount < minKeypoints) {
      return {
        isValid: false,
        reason: `Insufficient keypoints: ${keypointCount} (min: ${minKeypoints}) - likely covered face or photo`,
        score: keypointCount / minKeypoints
      };
    }

    // STRICT FACE VALIDATION: Check keypoint distribution to ensure it's actually a face
    if (keypoints && keypoints.length >= 4) {
      const xCoords = keypoints.map(kp => kp.x);
      const yCoords = keypoints.map(kp => kp.y);

      // Check if keypoints are distributed like facial features (not clustered like a hand)
      const xRange = Math.max(...xCoords) - Math.min(...xCoords);
      const yRange = Math.max(...yCoords) - Math.min(...yCoords);
      // Better spread calculation for upright faces - average both dimensions instead of minimum
      const xSpread = xRange / box.width;
      const ySpread = yRange / box.height;
      const keypointSpread = (xSpread + ySpread) / 2;

      // More lenient keypoint spread for natural upright faces - 35% is sufficient
      if (keypointSpread < 0.35) {
        return {
          isValid: false,
          reason: `Keypoints too clustered (${Math.round(keypointSpread * 100)}%) - likely covered face, photo, or non-face object`,
          score: keypointSpread
        };
      }

      // Check keypoint density - faces have well-distributed keypoints
      const centerX = xCoords.reduce((a, b) => a + b, 0) / xCoords.length;
      const centerY = yCoords.reduce((a, b) => a + b, 0) / yCoords.length;

      // Calculate how many keypoints are in different regions (eyes, nose, mouth areas)
      // Make regions much larger and more forgiving for real-world usage
      const topRegion = keypoints.filter(kp => kp.y < centerY - box.height * 0.1).length;
      const middleRegion = keypoints.filter(kp => Math.abs(kp.y - centerY) <= box.height * 0.4).length;
      const bottomRegion = keypoints.filter(kp => kp.y > centerY + box.height * 0.1).length;

      // STRICT: Need keypoints across ALL face regions (eyes, nose, mouth)
      const regionsWithKeypoints = (topRegion > 0 ? 1 : 0) + (middleRegion > 0 ? 1 : 0) + (bottomRegion > 0 ? 1 : 0);

      // More forgiving for upright faces - require 2 regions OR good middle concentration
      const hasGoodMiddleConcentration = middleRegion >= Math.floor(keypointCount * 0.4); // 40% in middle
      if (regionsWithKeypoints < 2 && !hasGoodMiddleConcentration) {
        return {
          isValid: false,
          reason: `Poor face feature distribution - need 2+ regions or 40%+ in middle (top:${topRegion}, mid:${middleRegion}, bot:${bottomRegion})`,
          score: 0.3
        };
      }
    }

    // Check face size (heavily optimized for mobile devices - phones/tablets)
    const faceWidth = box.width;
    const faceHeight = box.height;
    const minFaceSize = 20; // Very small minimum for mobile cameras - allows distant faces

    if (faceWidth < minFaceSize || faceHeight < minFaceSize) {
      return {
        isValid: false,
        reason: `Face too small: ${Math.round(faceWidth)}x${Math.round(faceHeight)} (min: ${minFaceSize}x${minFaceSize})`,
        score: Math.min(faceWidth, faceHeight) / minFaceSize
      };
    }

    // Check if face is mostly within frame (at least 90% visible)
    const leftEdge = box.xMin;
    const rightEdge = box.xMin + box.width;
    const topEdge = box.yMin;
    const bottomEdge = box.yMin + box.height;

    const isNearEdge = (
      leftEdge < 5 ||
      rightEdge > imageWidth - 5 ||
      topEdge < 5 ||
      bottomEdge > imageHeight - 5
    );

    if (isNearEdge) {
      return {
        isValid: false,
        reason: 'Face too close to frame edge (partial face)',
        score: 0.3
      };
    }

    // Check face aspect ratio (should be roughly rectangular, more strict for faces)
    const aspectRatio = faceWidth / faceHeight;
    const minAspectRatio = 0.75; // More strict
    const maxAspectRatio = 1.3;  // More strict

    if (aspectRatio < minAspectRatio || aspectRatio > maxAspectRatio) {
      return {
        isValid: false,
        reason: `Unusual face aspect ratio: ${aspectRatio.toFixed(2)} (should be between ${minAspectRatio}-${maxAspectRatio}) - may not be a face`,
        score: 0.4
      };
    }

    // Additional face size validation - optimized for mobile devices
    const faceArea = faceWidth * faceHeight;
    const imageArea = imageWidth * imageHeight;
    const faceRatio = faceArea / imageArea;

    // Very mobile-friendly: Face can be between 0.5% and 80% of image area
    // Allows very distant faces (0.5%) and very close faces (80%)
    if (faceRatio < 0.005 || faceRatio > 0.8) {
      return {
        isValid: false,
        reason: `Face size ratio unusual: ${(faceRatio * 100).toFixed(1)}% of image (should be 0.5-80% for mobile)`,
        score: faceRatio < 0.005 ? faceRatio / 0.005 : 0.8 / faceRatio
      };
    }

    // Calculate quality score based on all factors - optimized for upright natural faces
    const sizeScore = Math.min(1, Math.min(faceWidth, faceHeight) / 120); // More lenient size requirement
    const positionScore = 1; // Face is well-positioned (passed edge checks)
    const keypointScore = Math.min(1, keypointCount / 12); // More lenient keypoint requirement for upright faces
    // Normal upright faces are typically 0.8-1.0 ratio (taller than wide)
    const idealAspectRatio = 0.9; // Natural upright face ratio
    const aspectScore = Math.max(0, 1 - Math.abs(aspectRatio - idealAspectRatio) / 0.4); // More forgiving range
    const areaScore = Math.min(1, faceRatio / 0.06); // More lenient area requirement for upright faces

    // Weight the scores to favor natural upright position
    const overallScore = (sizeScore * 0.15 + positionScore * 0.25 + keypointScore * 0.2 + aspectScore * 0.2 + areaScore * 0.2);

    // Debug logging for quality score components
    console.log('üîç Quality Score Breakdown:', {
      aspectRatio: aspectRatio.toFixed(3),
      sizeScore: sizeScore.toFixed(3),
      positionScore: positionScore.toFixed(3),
      keypointScore: keypointScore.toFixed(3),
      aspectScore: aspectScore.toFixed(3),
      areaScore: areaScore.toFixed(3),
      overallScore: overallScore.toFixed(3)
    });

    // Balanced quality requirement - strict enough to reject photos but allow natural upright faces
    if (overallScore < 0.45) {
      return {
        isValid: false,
        reason: `Overall quality too low: ${Math.round(overallScore * 100)}% (need 45%+) - likely photo or covered face`,
        score: overallScore
      };
    }

    return {
      isValid: true,
      score: overallScore,
      reason: `High quality face detected: ${Math.round(overallScore * 100)}% score`
    };
  }

  // FAST face cropping without base64 - returns canvas for direct processing
  cropFaceFromVideo(videoElement: HTMLVideoElement, face: faceDetection.Face, padding = 0.3): HTMLCanvasElement | null {
    const { box } = face;
    if (!box || !this.canvas || !this.ctx) {
      console.error('Cannot crop face - missing box or canvas');
      return null;
    }

    // Validate video dimensions
    if (!videoElement.videoWidth || !videoElement.videoHeight) {
      console.error('Invalid video dimensions:', { width: videoElement.videoWidth, height: videoElement.videoHeight });
      return null;
    }

    // Add padding around the face for better recognition
    const paddingX = box.width * padding;
    const paddingY = box.height * padding;

    const cropX = Math.max(0, box.xMin - paddingX);
    const cropY = Math.max(0, box.yMin - paddingY);
    const cropWidth = Math.min(videoElement.videoWidth - cropX, box.width + (paddingX * 2));
    const cropHeight = Math.min(videoElement.videoHeight - cropY, box.height + (paddingY * 2));

    // Validate crop dimensions
    if (cropWidth <= 0 || cropHeight <= 0) {
      console.error('Invalid crop dimensions:', { cropWidth, cropHeight, box, videoSize: { width: videoElement.videoWidth, height: videoElement.videoHeight } });
      return null;
    }

    // Set canvas to crop dimensions
    this.canvas.width = cropWidth;
    this.canvas.height = cropHeight;

    // Clear and draw the cropped face
    this.ctx.clearRect(0, 0, cropWidth, cropHeight);
    this.ctx.drawImage(
      videoElement,
      cropX, cropY, cropWidth, cropHeight,  // Source rectangle
      0, 0, cropWidth, cropHeight           // Destination rectangle
    );

    // Enhance the cropped face for better recognition
    // Double-check dimensions before getImageData (should already be validated above)
    if (cropWidth <= 0 || cropHeight <= 0) {
      console.error('Invalid dimensions for face enhancement:', { cropWidth, cropHeight });
      return this.canvas.toDataURL('image/jpeg', 0.95);
    }

    const imageData = this.ctx.getImageData(0, 0, cropWidth, cropHeight);
    const data = imageData.data;

    // Apply face-specific enhancement
    const brightnessBoost = 20;
    const contrastFactor = 1.3;
    const saturationBoost = 1.1;

    for (let i = 0; i < data.length; i += 4) {
      const r = data[i];
      const g = data[i + 1];
      const b = data[i + 2];

      // Apply contrast and brightness
      let newR = ((r - 128) * contrastFactor + 128) + brightnessBoost;
      let newG = ((g - 128) * contrastFactor + 128) + brightnessBoost;
      let newB = ((b - 128) * contrastFactor + 128) + brightnessBoost;

      // Apply subtle saturation boost for better feature definition
      const gray = 0.299 * newR + 0.587 * newG + 0.114 * newB;
      newR = gray + (newR - gray) * saturationBoost;
      newG = gray + (newG - gray) * saturationBoost;
      newB = gray + (newB - gray) * saturationBoost;

      // Clamp values
      data[i] = Math.max(0, Math.min(255, newR));
      data[i + 1] = Math.max(0, Math.min(255, newG));
      data[i + 2] = Math.max(0, Math.min(255, newB));
    }

    // Put enhanced data back
    this.ctx.putImageData(imageData, 0, 0);

    console.log('‚úÖ Fast face crop completed:', {
      original: `${videoElement.videoWidth}x${videoElement.videoHeight}`,
      cropped: `${cropWidth}x${cropHeight}`,
      skipBaseBase64: true
    });

    return this.canvas; // Return canvas directly for faster processing
  }

  // Validate that captured image actually contains a face (not just body or hand)
  async validateCapturedFace(imageDataUrl: string): Promise<boolean> {
    try {
      // Create image element from data URL
      const img = new Image();
      img.src = imageDataUrl;

      await new Promise((resolve, reject) => {
        img.onload = resolve;
        img.onerror = reject;
      });

      // Try to detect faces in the captured image
      const faces = await this.detectFaces(img);

      // Image should contain at least one face and face should be reasonably sized
      if (faces.length > 0) {
        const face = faces[0];
        const faceArea = face.box.width * face.box.height;
        const imageArea = img.width * img.height;
        const faceRatio = faceArea / imageArea;

        // Use the enhanced face quality validation to ensure it's a real face
        const qualityCheck = this.validateFaceQuality(face, img.width, img.height);

        // Very mobile-friendly validation
        // Face should take up at least 2% of the cropped image (very lenient for mobile)
        const hasGoodSize = faceRatio > 0.02;
        // Use quality check but with lowered requirements for mobile
        const hasGoodQuality = qualityCheck.isValid;

        const isValidFace = hasGoodSize && hasGoodQuality;

        console.log('üîç DETAILED FACE VALIDATION RESULT:', {
          facesDetected: faces.length,
          faceRatio: faceRatio.toFixed(3),
          hasGoodSize: hasGoodSize,
          sizeThreshold: 0.25,
          hasGoodQuality: hasGoodQuality,
          qualityCheck: qualityCheck,
          finalResult: isValidFace,
          imageSize: { width: img.width, height: img.height },
          faceBox: face.box
        });

        return isValidFace;
      }

      console.warn('No faces detected in captured image - likely captured hand/body instead of face');
      return false;
    } catch (error) {
      console.error('Face validation failed:', error);
      return false; // If validation fails, reject the image
    }
  }

  // ULTRA-STRICT face matching with multiple verification layers
  async matchFace(faceEmbedding: number[], storedEmbeddings: { id: string; embedding: number[]; name: string; status: string }[], baseThreshold = 0.98) {
    if (storedEmbeddings.length === 0) {
      console.log('üîç No stored embeddings to match against');
      return null;
    }

    let bestMatch = null;
    let bestSimilarity = 0;
    let secondBestSimilarity = 0;
    let thirdBestSimilarity = 0;
    const allScores: Array<{id: string, name: string, similarity: number}> = [];

    console.log(`üîç ULTRA-STRICT matching against ${storedEmbeddings.length} stored faces with threshold ${baseThreshold}`);

    // Calculate similarities with all stored faces
    for (const stored of storedEmbeddings) {
      const similarity = this.calculateSimilarity(faceEmbedding, stored.embedding);
      allScores.push({ id: stored.id, name: stored.name, similarity });

      console.log(`üîç Similarity with ${stored.name}: ${similarity.toFixed(4)}`);

      if (similarity > bestSimilarity) {
        thirdBestSimilarity = secondBestSimilarity;
        secondBestSimilarity = bestSimilarity;
        bestSimilarity = similarity;
        bestMatch = {
          ...stored,
          confidence: similarity
        };
      } else if (similarity > secondBestSimilarity) {
        thirdBestSimilarity = secondBestSimilarity;
        secondBestSimilarity = similarity;
      } else if (similarity > thirdBestSimilarity) {
        thirdBestSimilarity = similarity;
      }
    }

    // Sort scores to see distribution
    allScores.sort((a, b) => b.similarity - a.similarity);
    console.log('üèÜ Top 5 matches:', allScores.slice(0, 5));

    // ULTRA-STRICT verification with multiple checks
    if (bestMatch && bestSimilarity >= baseThreshold) {

      // 1. STRICT CONFIDENCE GAP - Must be significantly better than second best
      const confidenceGap = bestSimilarity - secondBestSimilarity;
      const strictMinGap = 0.15; // Minimum 15% difference (3x stricter)

      console.log(`üîç Gap Analysis: 1st=${bestSimilarity.toFixed(4)}, 2nd=${secondBestSimilarity.toFixed(4)}, 3rd=${thirdBestSimilarity.toFixed(4)}, gap=${confidenceGap.toFixed(4)}`);

      if (confidenceGap < strictMinGap) {
        console.log(`‚ùå REJECTED: Insufficient confidence gap ${confidenceGap.toFixed(4)} < ${strictMinGap} - faces too similar, could be family members`);
        return null;
      }

      // 2. ABSOLUTE MINIMUM THRESHOLD - No adaptive lowering
      const absoluteMinThreshold = 0.97; // Never go below this - much stricter
      if (bestSimilarity < absoluteMinThreshold) {
        console.log(`‚ùå REJECTED: Below absolute minimum ${bestSimilarity.toFixed(4)} < ${absoluteMinThreshold}`);
        return null;
      }

      // 3. DISTRIBUTION CHECK - Ensure this match is clearly superior to all others
      const avgOtherScores = allScores.slice(1, 4).reduce((sum, score) => sum + score.similarity, 0) / Math.min(3, allScores.length - 1);
      const distributionGap = bestSimilarity - avgOtherScores;
      const minDistributionGap = 0.20; // Best match must be 20% better than average of others

      if (distributionGap < minDistributionGap) {
        console.log(`‚ùå REJECTED: Poor distribution gap ${distributionGap.toFixed(4)} < ${minDistributionGap} - not uniquely similar`);
        return null;
      }

      // 4. EMBEDDING QUALITY CHECK - Ensure embeddings are sufficiently different
      if (this.areEmbeddingsTooSimilar(faceEmbedding, storedEmbeddings.map(s => s.embedding).filter(e => e !== bestMatch.embedding))) {
        console.log(`‚ùå REJECTED: Face embeddings suggest family resemblance or insufficient uniqueness`);
        return null;
      }

      // 5. FINAL ULTRA-STRICT CHECK - All conditions must be met
      const ultraStrictPassed = (
        bestSimilarity >= 0.98 &&           // Extremely high similarity required
        confidenceGap >= 0.20 &&            // Much larger gap required
        distributionGap >= 0.25 &&           // Must strongly stand out from others
        bestSimilarity < 0.999               // Prevent perfect matches (likely duplicates)
      );

      if (ultraStrictPassed) {
        console.log(`‚úÖ ULTRA-STRICT MATCH CONFIRMED: ${bestMatch.name} with confidence ${bestSimilarity.toFixed(4)}`);
        console.log(`‚úÖ Verification: gap=${confidenceGap.toFixed(4)}, distribution=${distributionGap.toFixed(4)}`);
        return bestMatch;
      } else {
        console.log(`‚ùå FAILED ultra-strict verification despite high similarity`);
      }
    } else {
      console.log(`‚ùå No match above base threshold: best=${bestSimilarity.toFixed(4)} < ${baseThreshold}`);
    }

    return null; // No ultra-strict confident match found
  }

  // Check if embeddings are too similar (family members detection)
  private areEmbeddingsTooSimilar(newEmbedding: number[], otherEmbeddings: number[][]): boolean {
    if (otherEmbeddings.length === 0) return false;

    // Calculate how similar this face is to the general population of stored faces
    const similarities = otherEmbeddings.map(embedding => this.calculateSimilarity(newEmbedding, embedding));
    const avgSimilarity = similarities.reduce((sum, sim) => sum + sim, 0) / similarities.length;
    const maxSimilarity = Math.max(...similarities);

    // If average similarity is too high, might be family member
    const familyThreshold = 0.75; // If generally similar to stored faces, could be family
    const maxFamilyThreshold = 0.90; // If very similar to any other face, likely family

    console.log(`üë• Family resemblance check: avg=${avgSimilarity.toFixed(4)}, max=${maxSimilarity.toFixed(4)}`);

    return avgSimilarity > familyThreshold || maxSimilarity > maxFamilyThreshold;
  }

  // NEW: Load members from cache with fallback to Supabase
  async loadMembersFromCache(organizationId: string): Promise<CachedFaceEmbedding[]> {
    console.log(`üì¶ Loading face embeddings for organization: ${organizationId}`);

    // Try to get from cache first
    const cached = faceEmbeddingCache.getCachedEmbeddings(organizationId);
    if (cached) {
      console.log(`üì¶ Using cached embeddings: ${cached.length} faces`);
      return cached;
    }

    // Cache miss - load from Supabase
    console.log('üì° Cache miss - loading from Supabase...');
    try {
      const { data, error } = await supabase
        .from('members')
        .select('id, name, face_embedding, status, organization_id, updated_at')
        .eq('organization_id', organizationId)
        .not('face_embedding', 'is', null);

      if (error) throw error;

      const embeddings: CachedFaceEmbedding[] = (data || []).map(member => ({
        id: member.id,
        name: member.name,
        embedding: member.face_embedding,
        status: member.status,
        organization_id: member.organization_id,
        updated_at: member.updated_at
      }));

      // Cache the results
      faceEmbeddingCache.setCachedEmbeddings(organizationId, embeddings);

      console.log(`üì° Loaded ${embeddings.length} embeddings from Supabase and cached them`);
      return embeddings;
    } catch (error) {
      console.error('Failed to load embeddings from Supabase:', error);
      return [];
    }
  }

  // NEW: Enhanced match face with caching
  async matchFaceWithCache(faceEmbedding: number[], organizationId: string, baseThreshold = 0.95) {
    const cachedEmbeddings = await this.loadMembersFromCache(organizationId);

    if (cachedEmbeddings.length === 0) {
      console.log('üîç No cached embeddings available for matching');
      return null;
    }

    // Convert cached embeddings to the format expected by existing matchFace method
    const storedEmbeddings = cachedEmbeddings.map(cached => ({
      id: cached.id,
      embedding: cached.embedding,
      name: cached.name,
      status: cached.status
    }));

    return this.matchFace(faceEmbedding, storedEmbeddings, baseThreshold);
  }

  // NEW: Store face embedding with cache update
  async storeFaceEmbeddingWithCache(memberId: string, embedding: number[], organizationId: string): Promise<void> {
    try {
      // Store in Supabase
      const { error } = await supabase
        .from('members')
        .update({ face_embedding: embedding })
        .eq('id', memberId);

      if (error) throw error;

      // Update cache
      const { data: memberData } = await supabase
        .from('members')
        .select('*')
        .eq('id', memberId)
        .single();

      if (memberData) {
        const cachedEmbedding: CachedFaceEmbedding = {
          id: memberData.id,
          name: memberData.name,
          embedding: memberData.face_embedding,
          status: memberData.status,
          organization_id: memberData.organization_id,
          updated_at: memberData.updated_at
        };

        faceEmbeddingCache.addEmbedding(organizationId, cachedEmbedding);
      }

      console.log(`üíæ Stored face embedding for member ${memberId} and updated cache`);
    } catch (error) {
      console.error('Failed to store face embedding with cache:', error);
      throw error;
    }
  }

  // NEW: Refresh cache manually
  async refreshCache(organizationId: string): Promise<void> {
    console.log(`üîÑ Manually refreshing cache for organization ${organizationId}`);
    faceEmbeddingCache.clearCache(organizationId);
    await this.loadMembersFromCache(organizationId);
  }

  // NEW: Get cache statistics
  getCacheStats(organizationId: string): { cached: boolean; count: number; age: number } {
    const info = faceEmbeddingCache.getCacheInfo(organizationId);
    const cached = faceEmbeddingCache.getCachedEmbeddings(organizationId);

    return {
      cached: !!cached,
      count: cached?.length || 0,
      age: info ? Math.round((Date.now() - info.lastUpdated) / (60 * 1000)) : 0
    };
  }
}

export const faceRecognitionService = new FaceRecognitionService();